# This file is automatically generated. Any manual changes will be wiped out.
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  name: kaas-rocket-onprem-daemonsets-availability
  namespace: monitoring
  labels:
    application.giantswarm.io/team: rocket
    observability.giantswarm.io/tenant: giantswarm
spec:
  service: "onprem-daemonsets"
  labels:
    owner: "rocket"
    customer: "{{ .Values.managementCluster.customer }}"
    installation: "{{ .Values.managementCluster.name }}"
    pipeline: "{{ .Values.managementCluster.pipeline }}"
    {{- if .Values.managementCluster.region }}
    region: "{{ .Values.managementCluster.region }}"
    {{- end }}
  slos:
    - name: "availability"
      objective: 90
      description: "Onprem DaemonSet availability SLO"
      sli:
        events:
          errorQuery: |-
            {{`sum_over_time(kube_daemonset_status_number_unavailable{namespace=~"giantswarm|kube-system", daemonset=~"cert-exporter|kube-proxy|net-exporter|node-exporter", provider!~`}}{{ include "cloud.providers" .}}{{`}[{{.window}}])`}}
          totalQuery: |-
            {{`sum_over_time(kube_daemonset_status_desired_number_scheduled{namespace=~"giantswarm|kube-system", daemonset=~"cert-exporter|kube-proxy|net-exporter|node-exporter", provider!~`}}{{ include "cloud.providers" .}}{{`}[{{.window}}])`}}
      alerting:
        name: "OnpremDaemonSetAvailability"
        labels:
          cancel_if_monitoring_agent_down: "true"
          cancel_if_outside_working_hours: "true"
          area: kaas
          cluster_id: "{{ printf "{{ .Labels.cluster_id }}" }}"
          cluster_type: "{{ printf "{{ .Labels.cluster_id }}" }}"
          team: rocket
        pageAlert:
          disable: false
          labels:
            severity: page
        ticketAlert:
          disable: false
          labels:
            severity: "notify"
            slack_channel: "#alert-rocket"
